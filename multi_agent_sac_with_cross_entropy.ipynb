{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thAHoV4e0g7T",
        "outputId": "cd29df68-5363-4fc5-dbdb-6188a43dd0dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.11/dist-packages (2.37.0)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.11/dist-packages (7.34.0)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.11/dist-packages (2.6.1)\n",
            "Collecting pettingzoo[mpe]\n",
            "  Downloading pettingzoo-1.24.3-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from pettingzoo[mpe]) (2.0.2)\n",
            "Requirement already satisfied: gymnasium>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from pettingzoo[mpe]) (1.1.1)\n",
            "Collecting pygame\n",
            "  Downloading pygame-2.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Collecting jedi>=0.16 (from ipython)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython) (3.0.50)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython) (4.9.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=0.28.0->pettingzoo[mpe]) (3.1.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=0.28.0->pettingzoo[mpe]) (0.0.4)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython) (0.8.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython) (0.2.13)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading pygame-2.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pettingzoo-1.24.3-py3-none-any.whl (847 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m847.8/847.8 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pygame, jedi, pettingzoo\n",
            "  Attempting uninstall: pygame\n",
            "    Found existing installation: pygame 2.6.1\n",
            "    Uninstalling pygame-2.6.1:\n",
            "      Successfully uninstalled pygame-2.6.1\n",
            "Successfully installed jedi-0.19.2 pettingzoo-1.24.3 pygame-2.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pettingzoo[mpe] tensorflow matplotlib imageio ipython pygame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Md_tIS7u0cww",
        "outputId": "ad9b7715-a4c6-4675-e764-55340febd600"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU is available and will be used.\n",
            "Enabled GPU memory growth.\n",
            "TensorBoard logs will be saved to: masac_v2_tensorflow_logs_simple_spread_fixed_alpha_beta\n",
            "TensorFlow Version: 2.18.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pettingzoo/utils/conversions.py:144: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/pettingzoo/utils/conversions.py:158: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training (V2 Objective) for 5000 episodes...\n",
            "Settings: batch_size=512, start_steps=2000, update_every=1\n",
            "LRs: actor=0.0003, critic=0.001\n",
            "Fixed alpha=0.2, beta=0.1, tau=0.01, gamma=0.99, grad_clip=1.0\n",
            "Ep 10: Steps=100, Avg R (100)=-282.89, Buffer=1000, Tot Steps=1000, Time=0.9s\n",
            "Ep 20: Steps=100, Avg R (100)=-299.63, Buffer=2000, Tot Steps=2000, Time=11.6s\n",
            "Ep 30: Steps=100, Avg R (100)=-622.04, Buffer=3000, Tot Steps=3000, Time=35.1s\n",
            "Ep 40: Steps=100, Avg R (100)=-664.35, Buffer=4000, Tot Steps=4000, Time=57.9s\n",
            "Ep 50: Steps=100, Avg R (100)=-660.60, Buffer=5000, Tot Steps=5000, Time=80.8s\n",
            "Ep 60: Steps=100, Avg R (100)=-656.95, Buffer=6000, Tot Steps=6000, Time=104.2s\n",
            "Ep 70: Steps=100, Avg R (100)=-663.90, Buffer=7000, Tot Steps=7000, Time=127.3s\n",
            "Ep 80: Steps=100, Avg R (100)=-672.27, Buffer=8000, Tot Steps=8000, Time=150.2s\n",
            "Ep 90: Steps=100, Avg R (100)=-670.02, Buffer=9000, Tot Steps=9000, Time=173.4s\n",
            "Ep 100: Steps=100, Avg R (100)=-656.37, Buffer=10000, Tot Steps=10000, Time=197.1s\n",
            "Ep 110: Steps=100, Avg R (100)=-690.13, Buffer=11000, Tot Steps=11000, Time=220.4s\n",
            "Ep 120: Steps=100, Avg R (100)=-735.86, Buffer=12000, Tot Steps=12000, Time=243.6s\n",
            "Ep 130: Steps=100, Avg R (100)=-665.20, Buffer=13000, Tot Steps=13000, Time=267.0s\n",
            "Ep 140: Steps=100, Avg R (100)=-643.06, Buffer=14000, Tot Steps=14000, Time=290.4s\n",
            "Ep 150: Steps=100, Avg R (100)=-631.88, Buffer=15000, Tot Steps=15000, Time=313.7s\n",
            "Ep 160: Steps=100, Avg R (100)=-629.59, Buffer=16000, Tot Steps=16000, Time=337.5s\n",
            "Ep 170: Steps=100, Avg R (100)=-615.35, Buffer=17000, Tot Steps=17000, Time=361.0s\n",
            "Ep 180: Steps=100, Avg R (100)=-624.18, Buffer=18000, Tot Steps=18000, Time=384.3s\n",
            "Ep 190: Steps=100, Avg R (100)=-623.79, Buffer=19000, Tot Steps=19000, Time=407.4s\n",
            "Ep 200: Steps=100, Avg R (100)=-636.70, Buffer=20000, Tot Steps=20000, Time=430.9s\n",
            "Ep 210: Steps=100, Avg R (100)=-627.47, Buffer=21000, Tot Steps=21000, Time=453.9s\n",
            "Ep 220: Steps=100, Avg R (100)=-604.69, Buffer=22000, Tot Steps=22000, Time=476.8s\n",
            "Ep 230: Steps=100, Avg R (100)=-623.86, Buffer=23000, Tot Steps=23000, Time=501.3s\n",
            "Ep 240: Steps=100, Avg R (100)=-618.94, Buffer=24000, Tot Steps=24000, Time=525.3s\n",
            "Ep 250: Steps=100, Avg R (100)=-619.52, Buffer=25000, Tot Steps=25000, Time=549.8s\n",
            "Ep 260: Steps=100, Avg R (100)=-607.36, Buffer=26000, Tot Steps=26000, Time=573.4s\n",
            "Ep 270: Steps=100, Avg R (100)=-609.47, Buffer=27000, Tot Steps=27000, Time=597.6s\n",
            "Ep 280: Steps=100, Avg R (100)=-577.58, Buffer=28000, Tot Steps=28000, Time=622.1s\n",
            "Ep 290: Steps=100, Avg R (100)=-561.84, Buffer=29000, Tot Steps=29000, Time=646.7s\n",
            "Ep 300: Steps=100, Avg R (100)=-535.46, Buffer=30000, Tot Steps=30000, Time=670.8s\n",
            "Ep 310: Steps=100, Avg R (100)=-524.17, Buffer=31000, Tot Steps=31000, Time=695.2s\n",
            "Ep 320: Steps=100, Avg R (100)=-507.90, Buffer=32000, Tot Steps=32000, Time=719.1s\n",
            "Ep 330: Steps=100, Avg R (100)=-489.44, Buffer=33000, Tot Steps=33000, Time=742.6s\n",
            "Ep 340: Steps=100, Avg R (100)=-489.83, Buffer=34000, Tot Steps=34000, Time=767.9s\n",
            "Ep 350: Steps=100, Avg R (100)=-477.35, Buffer=35000, Tot Steps=35000, Time=793.1s\n",
            "Ep 360: Steps=100, Avg R (100)=-467.96, Buffer=36000, Tot Steps=36000, Time=819.5s\n",
            "Ep 370: Steps=100, Avg R (100)=-453.64, Buffer=37000, Tot Steps=37000, Time=844.8s\n",
            "Ep 380: Steps=100, Avg R (100)=-451.28, Buffer=38000, Tot Steps=38000, Time=869.7s\n",
            "Ep 390: Steps=100, Avg R (100)=-452.72, Buffer=39000, Tot Steps=39000, Time=894.1s\n",
            "Ep 400: Steps=100, Avg R (100)=-451.50, Buffer=40000, Tot Steps=40000, Time=918.6s\n",
            "Ep 410: Steps=100, Avg R (100)=-450.74, Buffer=41000, Tot Steps=41000, Time=943.6s\n",
            "Ep 420: Steps=100, Avg R (100)=-463.78, Buffer=42000, Tot Steps=42000, Time=968.6s\n",
            "Ep 430: Steps=100, Avg R (100)=-447.69, Buffer=43000, Tot Steps=43000, Time=993.8s\n",
            "Ep 440: Steps=100, Avg R (100)=-430.76, Buffer=44000, Tot Steps=44000, Time=1018.7s\n",
            "Ep 450: Steps=100, Avg R (100)=-425.01, Buffer=45000, Tot Steps=45000, Time=1042.9s\n",
            "Ep 460: Steps=100, Avg R (100)=-423.65, Buffer=46000, Tot Steps=46000, Time=1066.8s\n",
            "Ep 470: Steps=100, Avg R (100)=-425.08, Buffer=47000, Tot Steps=47000, Time=1091.1s\n",
            "Ep 480: Steps=100, Avg R (100)=-418.87, Buffer=48000, Tot Steps=48000, Time=1115.5s\n",
            "Ep 490: Steps=100, Avg R (100)=-412.14, Buffer=49000, Tot Steps=49000, Time=1140.1s\n",
            "Ep 500: Steps=100, Avg R (100)=-420.86, Buffer=50000, Tot Steps=50000, Time=1164.4s\n",
            "Ep 510: Steps=100, Avg R (100)=-431.40, Buffer=51000, Tot Steps=51000, Time=1189.0s\n",
            "Ep 520: Steps=100, Avg R (100)=-430.11, Buffer=52000, Tot Steps=52000, Time=1213.4s\n",
            "Ep 530: Steps=100, Avg R (100)=-454.34, Buffer=53000, Tot Steps=53000, Time=1237.3s\n",
            "Ep 540: Steps=100, Avg R (100)=-477.29, Buffer=54000, Tot Steps=54000, Time=1261.0s\n",
            "Ep 550: Steps=100, Avg R (100)=-490.95, Buffer=55000, Tot Steps=55000, Time=1284.4s\n",
            "Ep 560: Steps=100, Avg R (100)=-498.89, Buffer=56000, Tot Steps=56000, Time=1308.1s\n",
            "Ep 570: Steps=100, Avg R (100)=-499.83, Buffer=57000, Tot Steps=57000, Time=1331.9s\n",
            "Ep 580: Steps=100, Avg R (100)=-493.54, Buffer=58000, Tot Steps=58000, Time=1355.8s\n",
            "Ep 590: Steps=100, Avg R (100)=-480.62, Buffer=59000, Tot Steps=59000, Time=1379.7s\n",
            "Ep 600: Steps=100, Avg R (100)=-459.86, Buffer=60000, Tot Steps=60000, Time=1403.5s\n",
            "Ep 610: Steps=100, Avg R (100)=-438.95, Buffer=61000, Tot Steps=61000, Time=1427.0s\n",
            "Ep 620: Steps=100, Avg R (100)=-416.33, Buffer=62000, Tot Steps=62000, Time=1451.0s\n",
            "Ep 630: Steps=100, Avg R (100)=-382.90, Buffer=63000, Tot Steps=63000, Time=1474.9s\n",
            "Ep 640: Steps=100, Avg R (100)=-351.12, Buffer=64000, Tot Steps=64000, Time=1498.8s\n",
            "Ep 650: Steps=100, Avg R (100)=-334.35, Buffer=65000, Tot Steps=65000, Time=1522.5s\n",
            "Ep 660: Steps=100, Avg R (100)=-321.34, Buffer=66000, Tot Steps=66000, Time=1547.0s\n",
            "Ep 670: Steps=100, Avg R (100)=-301.07, Buffer=67000, Tot Steps=67000, Time=1571.6s\n",
            "Ep 680: Steps=100, Avg R (100)=-298.38, Buffer=68000, Tot Steps=68000, Time=1595.8s\n",
            "Ep 690: Steps=100, Avg R (100)=-308.74, Buffer=69000, Tot Steps=69000, Time=1619.9s\n",
            "Ep 700: Steps=100, Avg R (100)=-310.25, Buffer=70000, Tot Steps=70000, Time=1643.8s\n",
            "Ep 710: Steps=100, Avg R (100)=-309.85, Buffer=71000, Tot Steps=71000, Time=1667.7s\n",
            "Ep 720: Steps=100, Avg R (100)=-315.12, Buffer=72000, Tot Steps=72000, Time=1691.9s\n",
            "Ep 730: Steps=100, Avg R (100)=-311.82, Buffer=73000, Tot Steps=73000, Time=1716.1s\n",
            "Ep 740: Steps=100, Avg R (100)=-313.07, Buffer=74000, Tot Steps=74000, Time=1740.6s\n",
            "Ep 750: Steps=100, Avg R (100)=-307.14, Buffer=75000, Tot Steps=75000, Time=1765.1s\n",
            "Ep 760: Steps=100, Avg R (100)=-305.36, Buffer=76000, Tot Steps=76000, Time=1789.8s\n",
            "Ep 770: Steps=100, Avg R (100)=-309.27, Buffer=77000, Tot Steps=77000, Time=1813.6s\n",
            "Ep 780: Steps=100, Avg R (100)=-302.39, Buffer=78000, Tot Steps=78000, Time=1837.7s\n",
            "Ep 790: Steps=100, Avg R (100)=-286.35, Buffer=79000, Tot Steps=79000, Time=1862.0s\n",
            "Ep 800: Steps=100, Avg R (100)=-284.25, Buffer=80000, Tot Steps=80000, Time=1886.2s\n",
            "Ep 810: Steps=100, Avg R (100)=-282.44, Buffer=81000, Tot Steps=81000, Time=1910.3s\n",
            "Ep 820: Steps=100, Avg R (100)=-274.93, Buffer=82000, Tot Steps=82000, Time=1934.9s\n",
            "Ep 830: Steps=100, Avg R (100)=-274.79, Buffer=83000, Tot Steps=83000, Time=1959.5s\n",
            "Ep 840: Steps=100, Avg R (100)=-271.08, Buffer=84000, Tot Steps=84000, Time=1984.5s\n",
            "Ep 850: Steps=100, Avg R (100)=-268.34, Buffer=85000, Tot Steps=85000, Time=2009.3s\n",
            "Ep 860: Steps=100, Avg R (100)=-269.15, Buffer=86000, Tot Steps=86000, Time=2034.0s\n",
            "Ep 870: Steps=100, Avg R (100)=-268.61, Buffer=87000, Tot Steps=87000, Time=2058.4s\n",
            "Ep 880: Steps=100, Avg R (100)=-277.13, Buffer=88000, Tot Steps=88000, Time=2082.5s\n",
            "Ep 890: Steps=100, Avg R (100)=-286.15, Buffer=89000, Tot Steps=89000, Time=2106.3s\n",
            "Ep 900: Steps=100, Avg R (100)=-285.05, Buffer=90000, Tot Steps=90000, Time=2130.5s\n",
            "Ep 910: Steps=100, Avg R (100)=-284.69, Buffer=91000, Tot Steps=91000, Time=2154.8s\n",
            "Ep 920: Steps=100, Avg R (100)=-288.21, Buffer=92000, Tot Steps=92000, Time=2178.9s\n",
            "Ep 930: Steps=100, Avg R (100)=-286.14, Buffer=93000, Tot Steps=93000, Time=2203.3s\n",
            "Ep 940: Steps=100, Avg R (100)=-290.46, Buffer=94000, Tot Steps=94000, Time=2227.7s\n",
            "Ep 950: Steps=100, Avg R (100)=-296.10, Buffer=95000, Tot Steps=95000, Time=2251.9s\n",
            "Ep 960: Steps=100, Avg R (100)=-295.21, Buffer=96000, Tot Steps=96000, Time=2276.7s\n",
            "Ep 970: Steps=100, Avg R (100)=-296.12, Buffer=97000, Tot Steps=97000, Time=2301.2s\n",
            "Ep 980: Steps=100, Avg R (100)=-288.87, Buffer=98000, Tot Steps=98000, Time=2325.3s\n",
            "Ep 990: Steps=100, Avg R (100)=-284.55, Buffer=99000, Tot Steps=99000, Time=2349.7s\n",
            "Ep 1000: Steps=100, Avg R (100)=-283.71, Buffer=100000, Tot Steps=100000, Time=2374.2s\n",
            "Ep 1010: Steps=100, Avg R (100)=-283.96, Buffer=101000, Tot Steps=101000, Time=2398.7s\n",
            "Ep 1020: Steps=100, Avg R (100)=-283.72, Buffer=102000, Tot Steps=102000, Time=2423.0s\n",
            "Ep 1030: Steps=100, Avg R (100)=-292.26, Buffer=103000, Tot Steps=103000, Time=2447.5s\n",
            "Ep 1040: Steps=100, Avg R (100)=-297.06, Buffer=104000, Tot Steps=104000, Time=2471.9s\n",
            "Ep 1050: Steps=100, Avg R (100)=-296.13, Buffer=105000, Tot Steps=105000, Time=2496.4s\n",
            "Ep 1060: Steps=100, Avg R (100)=-290.03, Buffer=106000, Tot Steps=106000, Time=2520.9s\n",
            "Ep 1070: Steps=100, Avg R (100)=-284.69, Buffer=107000, Tot Steps=107000, Time=2545.9s\n",
            "Ep 1080: Steps=100, Avg R (100)=-289.17, Buffer=108000, Tot Steps=108000, Time=2570.4s\n",
            "Ep 1090: Steps=100, Avg R (100)=-285.70, Buffer=109000, Tot Steps=109000, Time=2594.8s\n",
            "Ep 1100: Steps=100, Avg R (100)=-295.25, Buffer=110000, Tot Steps=110000, Time=2619.3s\n",
            "Ep 1110: Steps=100, Avg R (100)=-293.24, Buffer=111000, Tot Steps=111000, Time=2643.9s\n",
            "Ep 1120: Steps=100, Avg R (100)=-294.10, Buffer=112000, Tot Steps=112000, Time=2668.6s\n",
            "Ep 1130: Steps=100, Avg R (100)=-288.86, Buffer=113000, Tot Steps=113000, Time=2693.0s\n",
            "Ep 1140: Steps=100, Avg R (100)=-283.57, Buffer=114000, Tot Steps=114000, Time=2717.9s\n",
            "Ep 1150: Steps=100, Avg R (100)=-283.15, Buffer=115000, Tot Steps=115000, Time=2743.4s\n",
            "Ep 1160: Steps=100, Avg R (100)=-284.90, Buffer=116000, Tot Steps=116000, Time=2769.1s\n",
            "Ep 1170: Steps=100, Avg R (100)=-287.04, Buffer=117000, Tot Steps=117000, Time=2794.9s\n",
            "Ep 1180: Steps=100, Avg R (100)=-285.89, Buffer=118000, Tot Steps=118000, Time=2820.4s\n",
            "Ep 1190: Steps=100, Avg R (100)=-286.83, Buffer=119000, Tot Steps=119000, Time=2845.6s\n",
            "Ep 1200: Steps=100, Avg R (100)=-276.02, Buffer=120000, Tot Steps=120000, Time=2870.5s\n",
            "Ep 1210: Steps=100, Avg R (100)=-272.11, Buffer=121000, Tot Steps=121000, Time=2895.8s\n",
            "Ep 1220: Steps=100, Avg R (100)=-272.86, Buffer=122000, Tot Steps=122000, Time=2921.4s\n",
            "Ep 1230: Steps=100, Avg R (100)=-276.89, Buffer=123000, Tot Steps=123000, Time=2946.6s\n",
            "Ep 1240: Steps=100, Avg R (100)=-277.33, Buffer=124000, Tot Steps=124000, Time=2971.4s\n",
            "Ep 1250: Steps=100, Avg R (100)=-277.50, Buffer=125000, Tot Steps=125000, Time=2996.5s\n",
            "Ep 1260: Steps=100, Avg R (100)=-275.87, Buffer=126000, Tot Steps=126000, Time=3021.2s\n",
            "Ep 1270: Steps=100, Avg R (100)=-279.91, Buffer=127000, Tot Steps=127000, Time=3046.0s\n",
            "Ep 1280: Steps=100, Avg R (100)=-279.18, Buffer=128000, Tot Steps=128000, Time=3071.5s\n",
            "Ep 1290: Steps=100, Avg R (100)=-281.38, Buffer=129000, Tot Steps=129000, Time=3096.7s\n",
            "Ep 1300: Steps=100, Avg R (100)=-289.37, Buffer=130000, Tot Steps=130000, Time=3122.5s\n",
            "Ep 1310: Steps=100, Avg R (100)=-295.91, Buffer=131000, Tot Steps=131000, Time=3147.7s\n",
            "Ep 1320: Steps=100, Avg R (100)=-297.86, Buffer=132000, Tot Steps=132000, Time=3172.4s\n",
            "Ep 1330: Steps=100, Avg R (100)=-296.61, Buffer=133000, Tot Steps=133000, Time=3197.2s\n",
            "Ep 1340: Steps=100, Avg R (100)=-295.75, Buffer=134000, Tot Steps=134000, Time=3222.1s\n",
            "Ep 1350: Steps=100, Avg R (100)=-295.62, Buffer=135000, Tot Steps=135000, Time=3246.7s\n",
            "Ep 1360: Steps=100, Avg R (100)=-299.95, Buffer=136000, Tot Steps=136000, Time=3271.8s\n",
            "Ep 1370: Steps=100, Avg R (100)=-301.03, Buffer=137000, Tot Steps=137000, Time=3297.4s\n",
            "Ep 1380: Steps=100, Avg R (100)=-305.49, Buffer=138000, Tot Steps=138000, Time=3322.3s\n",
            "Ep 1390: Steps=100, Avg R (100)=-302.50, Buffer=139000, Tot Steps=139000, Time=3347.7s\n",
            "Ep 1400: Steps=100, Avg R (100)=-298.28, Buffer=140000, Tot Steps=140000, Time=3373.0s\n",
            "Ep 1410: Steps=100, Avg R (100)=-298.54, Buffer=141000, Tot Steps=141000, Time=3398.0s\n",
            "Ep 1420: Steps=100, Avg R (100)=-294.49, Buffer=142000, Tot Steps=142000, Time=3423.0s\n",
            "Ep 1430: Steps=100, Avg R (100)=-291.71, Buffer=143000, Tot Steps=143000, Time=3449.0s\n",
            "Ep 1440: Steps=100, Avg R (100)=-291.44, Buffer=144000, Tot Steps=144000, Time=3474.6s\n",
            "Ep 1450: Steps=100, Avg R (100)=-291.34, Buffer=145000, Tot Steps=145000, Time=3500.0s\n",
            "Ep 1460: Steps=100, Avg R (100)=-288.69, Buffer=146000, Tot Steps=146000, Time=3525.2s\n",
            "Ep 1470: Steps=100, Avg R (100)=-284.13, Buffer=147000, Tot Steps=147000, Time=3550.0s\n",
            "Ep 1480: Steps=100, Avg R (100)=-284.13, Buffer=148000, Tot Steps=148000, Time=3575.1s\n",
            "Ep 1490: Steps=100, Avg R (100)=-289.97, Buffer=149000, Tot Steps=149000, Time=3600.7s\n",
            "Ep 1500: Steps=100, Avg R (100)=-296.34, Buffer=150000, Tot Steps=150000, Time=3626.3s\n",
            "Ep 1510: Steps=100, Avg R (100)=-303.36, Buffer=151000, Tot Steps=151000, Time=3651.5s\n",
            "Ep 1520: Steps=100, Avg R (100)=-328.18, Buffer=152000, Tot Steps=152000, Time=3677.0s\n",
            "Ep 1530: Steps=100, Avg R (100)=-345.68, Buffer=153000, Tot Steps=153000, Time=3702.6s\n",
            "Ep 1540: Steps=100, Avg R (100)=-482.32, Buffer=154000, Tot Steps=154000, Time=3727.9s\n",
            "Ep 1550: Steps=100, Avg R (100)=-851.38, Buffer=155000, Tot Steps=155000, Time=3753.3s\n",
            "Ep 1560: Steps=100, Avg R (100)=-1218.56, Buffer=156000, Tot Steps=156000, Time=3778.7s\n",
            "Ep 1570: Steps=100, Avg R (100)=-1595.55, Buffer=157000, Tot Steps=157000, Time=3804.1s\n",
            "Ep 1580: Steps=100, Avg R (100)=-1975.11, Buffer=158000, Tot Steps=158000, Time=3829.3s\n",
            "Ep 1590: Steps=100, Avg R (100)=-2354.39, Buffer=159000, Tot Steps=159000, Time=3854.9s\n",
            "Ep 1600: Steps=100, Avg R (100)=-2733.78, Buffer=160000, Tot Steps=160000, Time=3880.0s\n",
            "Ep 1610: Steps=100, Avg R (100)=-2923.47, Buffer=161000, Tot Steps=161000, Time=3905.1s\n",
            "Ep 1620: Steps=100, Avg R (100)=-3091.94, Buffer=162000, Tot Steps=162000, Time=3930.5s\n",
            "Ep 1630: Steps=100, Avg R (100)=-3407.47, Buffer=163000, Tot Steps=163000, Time=3955.8s\n",
            "Ep 1640: Steps=100, Avg R (100)=-3627.14, Buffer=164000, Tot Steps=164000, Time=3981.4s\n",
            "Ep 1650: Steps=100, Avg R (100)=-3490.93, Buffer=165000, Tot Steps=165000, Time=4006.5s\n",
            "Ep 1660: Steps=100, Avg R (100)=-3289.25, Buffer=166000, Tot Steps=166000, Time=4031.6s\n",
            "Ep 1670: Steps=100, Avg R (100)=-3167.76, Buffer=167000, Tot Steps=167000, Time=4056.8s\n",
            "Ep 1680: Steps=100, Avg R (100)=-3058.35, Buffer=168000, Tot Steps=168000, Time=4081.8s\n",
            "Ep 1690: Steps=100, Avg R (100)=-2938.93, Buffer=169000, Tot Steps=169000, Time=4106.9s\n",
            "Ep 1700: Steps=100, Avg R (100)=-2744.01, Buffer=170000, Tot Steps=170000, Time=4132.0s\n",
            "Ep 1710: Steps=100, Avg R (100)=-2714.42, Buffer=171000, Tot Steps=171000, Time=4157.2s\n",
            "Ep 1720: Steps=100, Avg R (100)=-2744.78, Buffer=172000, Tot Steps=172000, Time=4182.5s\n",
            "Ep 1730: Steps=100, Avg R (100)=-2587.07, Buffer=173000, Tot Steps=173000, Time=4207.6s\n",
            "Ep 1740: Steps=100, Avg R (100)=-2360.47, Buffer=174000, Tot Steps=174000, Time=4233.4s\n",
            "Ep 1750: Steps=100, Avg R (100)=-2364.96, Buffer=175000, Tot Steps=175000, Time=4259.1s\n",
            "Ep 1760: Steps=100, Avg R (100)=-2372.36, Buffer=176000, Tot Steps=176000, Time=4284.4s\n",
            "Ep 1770: Steps=100, Avg R (100)=-2415.05, Buffer=177000, Tot Steps=177000, Time=4309.9s\n",
            "Ep 1780: Steps=100, Avg R (100)=-2326.42, Buffer=178000, Tot Steps=178000, Time=4335.2s\n",
            "Ep 1790: Steps=100, Avg R (100)=-2217.43, Buffer=179000, Tot Steps=179000, Time=4360.6s\n",
            "Ep 1800: Steps=100, Avg R (100)=-2117.00, Buffer=180000, Tot Steps=180000, Time=4386.0s\n",
            "Ep 1810: Steps=100, Avg R (100)=-2014.63, Buffer=181000, Tot Steps=181000, Time=4412.0s\n",
            "Ep 1820: Steps=100, Avg R (100)=-1904.90, Buffer=182000, Tot Steps=182000, Time=4437.9s\n",
            "Ep 1830: Steps=100, Avg R (100)=-1851.72, Buffer=183000, Tot Steps=183000, Time=4464.8s\n",
            "Ep 1840: Steps=100, Avg R (100)=-1788.04, Buffer=184000, Tot Steps=184000, Time=4490.4s\n",
            "Ep 1850: Steps=100, Avg R (100)=-1627.90, Buffer=185000, Tot Steps=185000, Time=4515.8s\n",
            "Ep 1860: Steps=100, Avg R (100)=-1502.27, Buffer=186000, Tot Steps=186000, Time=4541.4s\n",
            "Ep 1870: Steps=100, Avg R (100)=-1216.52, Buffer=187000, Tot Steps=187000, Time=4567.3s\n",
            "Ep 1880: Steps=100, Avg R (100)=-1088.51, Buffer=188000, Tot Steps=188000, Time=4594.3s\n",
            "Ep 1890: Steps=100, Avg R (100)=-1194.86, Buffer=189000, Tot Steps=189000, Time=4621.8s\n",
            "Ep 1900: Steps=100, Avg R (100)=-1460.31, Buffer=190000, Tot Steps=190000, Time=4647.8s\n",
            "Ep 1910: Steps=100, Avg R (100)=-1692.99, Buffer=191000, Tot Steps=191000, Time=4674.8s\n",
            "Ep 1920: Steps=100, Avg R (100)=-1803.01, Buffer=192000, Tot Steps=192000, Time=4701.4s\n",
            "Ep 1930: Steps=100, Avg R (100)=-1815.07, Buffer=193000, Tot Steps=193000, Time=4727.8s\n",
            "Ep 1940: Steps=100, Avg R (100)=-1785.53, Buffer=194000, Tot Steps=194000, Time=4753.9s\n",
            "Ep 1950: Steps=100, Avg R (100)=-1719.49, Buffer=195000, Tot Steps=195000, Time=4780.1s\n",
            "Ep 1960: Steps=100, Avg R (100)=-1674.45, Buffer=196000, Tot Steps=196000, Time=4806.1s\n",
            "Ep 1970: Steps=100, Avg R (100)=-1663.92, Buffer=197000, Tot Steps=197000, Time=4831.9s\n",
            "Ep 1980: Steps=100, Avg R (100)=-1613.15, Buffer=198000, Tot Steps=198000, Time=4857.9s\n",
            "Ep 1990: Steps=100, Avg R (100)=-1412.19, Buffer=199000, Tot Steps=199000, Time=4883.6s\n",
            "Ep 2000: Steps=100, Avg R (100)=-1215.12, Buffer=200000, Tot Steps=200000, Time=4909.3s\n",
            "Ep 2010: Steps=100, Avg R (100)=-1046.18, Buffer=200000, Tot Steps=201000, Time=4936.1s\n",
            "Ep 2020: Steps=100, Avg R (100)=-932.30, Buffer=200000, Tot Steps=202000, Time=4963.0s\n",
            "Ep 2030: Steps=100, Avg R (100)=-869.01, Buffer=200000, Tot Steps=203000, Time=4989.5s\n",
            "Ep 2040: Steps=100, Avg R (100)=-899.32, Buffer=200000, Tot Steps=204000, Time=5015.5s\n",
            "Ep 2050: Steps=100, Avg R (100)=-942.56, Buffer=200000, Tot Steps=205000, Time=5041.6s\n",
            "Ep 2060: Steps=100, Avg R (100)=-1002.44, Buffer=200000, Tot Steps=206000, Time=5067.9s\n",
            "Ep 2070: Steps=100, Avg R (100)=-1034.95, Buffer=200000, Tot Steps=207000, Time=5094.6s\n",
            "Ep 2080: Steps=100, Avg R (100)=-1060.74, Buffer=200000, Tot Steps=208000, Time=5120.4s\n",
            "Ep 2090: Steps=100, Avg R (100)=-1003.49, Buffer=200000, Tot Steps=209000, Time=5146.1s\n",
            "Ep 2100: Steps=100, Avg R (100)=-850.66, Buffer=200000, Tot Steps=210000, Time=5172.1s\n",
            "Ep 2110: Steps=100, Avg R (100)=-719.71, Buffer=200000, Tot Steps=211000, Time=5198.4s\n",
            "Ep 2120: Steps=100, Avg R (100)=-611.12, Buffer=200000, Tot Steps=212000, Time=5224.4s\n",
            "Ep 2130: Steps=100, Avg R (100)=-543.04, Buffer=200000, Tot Steps=213000, Time=5250.2s\n",
            "Ep 2140: Steps=100, Avg R (100)=-472.68, Buffer=200000, Tot Steps=214000, Time=5276.3s\n",
            "Ep 2150: Steps=100, Avg R (100)=-416.30, Buffer=200000, Tot Steps=215000, Time=5301.9s\n",
            "Ep 2160: Steps=100, Avg R (100)=-353.51, Buffer=200000, Tot Steps=216000, Time=5327.9s\n",
            "Ep 2170: Steps=100, Avg R (100)=-325.90, Buffer=200000, Tot Steps=217000, Time=5353.7s\n",
            "Ep 2180: Steps=100, Avg R (100)=-302.13, Buffer=200000, Tot Steps=218000, Time=5379.5s\n",
            "Ep 2190: Steps=100, Avg R (100)=-306.06, Buffer=200000, Tot Steps=219000, Time=5405.3s\n",
            "Ep 2200: Steps=100, Avg R (100)=-326.21, Buffer=200000, Tot Steps=220000, Time=5431.5s\n",
            "Ep 2210: Steps=100, Avg R (100)=-339.43, Buffer=200000, Tot Steps=221000, Time=5457.5s\n",
            "Ep 2220: Steps=100, Avg R (100)=-360.89, Buffer=200000, Tot Steps=222000, Time=5483.5s\n",
            "Ep 2230: Steps=100, Avg R (100)=-375.69, Buffer=200000, Tot Steps=223000, Time=5509.1s\n",
            "Ep 2240: Steps=100, Avg R (100)=-405.77, Buffer=200000, Tot Steps=224000, Time=5537.7s\n",
            "Ep 2250: Steps=100, Avg R (100)=-419.26, Buffer=200000, Tot Steps=225000, Time=5563.3s\n",
            "Ep 2260: Steps=100, Avg R (100)=-442.33, Buffer=200000, Tot Steps=226000, Time=5589.4s\n",
            "Ep 2270: Steps=100, Avg R (100)=-450.35, Buffer=200000, Tot Steps=227000, Time=5615.6s\n",
            "Ep 2280: Steps=100, Avg R (100)=-454.45, Buffer=200000, Tot Steps=228000, Time=5641.1s\n",
            "Ep 2290: Steps=100, Avg R (100)=-460.88, Buffer=200000, Tot Steps=229000, Time=5666.7s\n",
            "Ep 2300: Steps=100, Avg R (100)=-453.39, Buffer=200000, Tot Steps=230000, Time=5692.3s\n",
            "Ep 2310: Steps=100, Avg R (100)=-458.60, Buffer=200000, Tot Steps=231000, Time=5717.7s\n",
            "Ep 2320: Steps=100, Avg R (100)=-456.17, Buffer=200000, Tot Steps=232000, Time=5744.8s\n",
            "Ep 2330: Steps=100, Avg R (100)=-453.15, Buffer=200000, Tot Steps=233000, Time=5772.1s\n",
            "Ep 2340: Steps=100, Avg R (100)=-450.78, Buffer=200000, Tot Steps=234000, Time=5797.7s\n",
            "Ep 2350: Steps=100, Avg R (100)=-454.87, Buffer=200000, Tot Steps=235000, Time=5823.7s\n",
            "Ep 2360: Steps=100, Avg R (100)=-436.83, Buffer=200000, Tot Steps=236000, Time=5849.8s\n",
            "Ep 2370: Steps=100, Avg R (100)=-427.04, Buffer=200000, Tot Steps=237000, Time=5876.2s\n",
            "Ep 2380: Steps=100, Avg R (100)=-423.73, Buffer=200000, Tot Steps=238000, Time=5902.4s\n",
            "Ep 2390: Steps=100, Avg R (100)=-419.67, Buffer=200000, Tot Steps=239000, Time=5929.2s\n",
            "Ep 2400: Steps=100, Avg R (100)=-410.83, Buffer=200000, Tot Steps=240000, Time=5955.5s\n",
            "Ep 2410: Steps=100, Avg R (100)=-407.16, Buffer=200000, Tot Steps=241000, Time=5981.5s\n",
            "Ep 2420: Steps=100, Avg R (100)=-402.63, Buffer=200000, Tot Steps=242000, Time=6007.7s\n",
            "Ep 2430: Steps=100, Avg R (100)=-396.52, Buffer=200000, Tot Steps=243000, Time=6034.0s\n",
            "Ep 2440: Steps=100, Avg R (100)=-388.15, Buffer=200000, Tot Steps=244000, Time=6061.1s\n",
            "Ep 2450: Steps=100, Avg R (100)=-388.26, Buffer=200000, Tot Steps=245000, Time=6087.4s\n",
            "Ep 2460: Steps=100, Avg R (100)=-400.96, Buffer=200000, Tot Steps=246000, Time=6113.7s\n",
            "Ep 2470: Steps=100, Avg R (100)=-417.73, Buffer=200000, Tot Steps=247000, Time=6140.2s\n",
            "Ep 2480: Steps=100, Avg R (100)=-411.12, Buffer=200000, Tot Steps=248000, Time=6166.8s\n",
            "Ep 2490: Steps=100, Avg R (100)=-413.22, Buffer=200000, Tot Steps=249000, Time=6192.7s\n",
            "Ep 2500: Steps=100, Avg R (100)=-407.97, Buffer=200000, Tot Steps=250000, Time=6218.6s\n",
            "Ep 2510: Steps=100, Avg R (100)=-411.35, Buffer=200000, Tot Steps=251000, Time=6245.0s\n",
            "Ep 2520: Steps=100, Avg R (100)=-393.53, Buffer=200000, Tot Steps=252000, Time=6271.6s\n",
            "Ep 2530: Steps=100, Avg R (100)=-397.73, Buffer=200000, Tot Steps=253000, Time=6297.3s\n",
            "Ep 2540: Steps=100, Avg R (100)=-388.68, Buffer=200000, Tot Steps=254000, Time=6322.9s\n",
            "Ep 2550: Steps=100, Avg R (100)=-379.63, Buffer=200000, Tot Steps=255000, Time=6348.9s\n",
            "Ep 2560: Steps=100, Avg R (100)=-380.61, Buffer=200000, Tot Steps=256000, Time=6375.9s\n",
            "Ep 2570: Steps=100, Avg R (100)=-368.82, Buffer=200000, Tot Steps=257000, Time=6402.8s\n",
            "Ep 2580: Steps=100, Avg R (100)=-376.82, Buffer=200000, Tot Steps=258000, Time=6429.7s\n",
            "Ep 2590: Steps=100, Avg R (100)=-384.26, Buffer=200000, Tot Steps=259000, Time=6455.8s\n",
            "Ep 2600: Steps=100, Avg R (100)=-402.21, Buffer=200000, Tot Steps=260000, Time=6482.0s\n",
            "Ep 2610: Steps=100, Avg R (100)=-392.29, Buffer=200000, Tot Steps=261000, Time=6508.7s\n",
            "Ep 2620: Steps=100, Avg R (100)=-399.96, Buffer=200000, Tot Steps=262000, Time=6534.8s\n",
            "Ep 2630: Steps=100, Avg R (100)=-398.11, Buffer=200000, Tot Steps=263000, Time=6560.7s\n",
            "Ep 2640: Steps=100, Avg R (100)=-402.83, Buffer=200000, Tot Steps=264000, Time=6586.7s\n",
            "Ep 2650: Steps=100, Avg R (100)=-410.84, Buffer=200000, Tot Steps=265000, Time=6612.5s\n",
            "Ep 2660: Steps=100, Avg R (100)=-410.32, Buffer=200000, Tot Steps=266000, Time=6638.2s\n",
            "Ep 2670: Steps=100, Avg R (100)=-410.91, Buffer=200000, Tot Steps=267000, Time=6664.2s\n",
            "Ep 2680: Steps=100, Avg R (100)=-405.40, Buffer=200000, Tot Steps=268000, Time=6690.6s\n",
            "Ep 2690: Steps=100, Avg R (100)=-389.84, Buffer=200000, Tot Steps=269000, Time=6716.9s\n",
            "Ep 2700: Steps=100, Avg R (100)=-375.01, Buffer=200000, Tot Steps=270000, Time=6742.7s\n",
            "Ep 2710: Steps=100, Avg R (100)=-375.06, Buffer=200000, Tot Steps=271000, Time=6768.6s\n",
            "Ep 2720: Steps=100, Avg R (100)=-383.06, Buffer=200000, Tot Steps=272000, Time=6794.6s\n",
            "Ep 2730: Steps=100, Avg R (100)=-382.71, Buffer=200000, Tot Steps=273000, Time=6820.7s\n",
            "Ep 2740: Steps=100, Avg R (100)=-379.98, Buffer=200000, Tot Steps=274000, Time=6847.5s\n",
            "Ep 2750: Steps=100, Avg R (100)=-375.95, Buffer=200000, Tot Steps=275000, Time=6873.7s\n",
            "Ep 2760: Steps=100, Avg R (100)=-364.60, Buffer=200000, Tot Steps=276000, Time=6899.5s\n",
            "Ep 2770: Steps=100, Avg R (100)=-367.79, Buffer=200000, Tot Steps=277000, Time=6925.3s\n",
            "Ep 2780: Steps=100, Avg R (100)=-374.10, Buffer=200000, Tot Steps=278000, Time=6951.1s\n",
            "Ep 2790: Steps=100, Avg R (100)=-377.98, Buffer=200000, Tot Steps=279000, Time=6976.8s\n",
            "Ep 2800: Steps=100, Avg R (100)=-376.74, Buffer=200000, Tot Steps=280000, Time=7002.7s\n",
            "Ep 2810: Steps=100, Avg R (100)=-378.25, Buffer=200000, Tot Steps=281000, Time=7029.5s\n",
            "Ep 2820: Steps=100, Avg R (100)=-369.36, Buffer=200000, Tot Steps=282000, Time=7055.3s\n",
            "Ep 2830: Steps=100, Avg R (100)=-362.02, Buffer=200000, Tot Steps=283000, Time=7081.0s\n",
            "Ep 2840: Steps=100, Avg R (100)=-357.47, Buffer=200000, Tot Steps=284000, Time=7106.9s\n",
            "Ep 2850: Steps=100, Avg R (100)=-357.18, Buffer=200000, Tot Steps=285000, Time=7132.7s\n",
            "Ep 2860: Steps=100, Avg R (100)=-355.22, Buffer=200000, Tot Steps=286000, Time=7158.4s\n",
            "Ep 2870: Steps=100, Avg R (100)=-354.04, Buffer=200000, Tot Steps=287000, Time=7184.2s\n",
            "Ep 2880: Steps=100, Avg R (100)=-352.60, Buffer=200000, Tot Steps=288000, Time=7210.7s\n",
            "Ep 2890: Steps=100, Avg R (100)=-363.78, Buffer=200000, Tot Steps=289000, Time=7236.4s\n",
            "Ep 2900: Steps=100, Avg R (100)=-380.52, Buffer=200000, Tot Steps=290000, Time=7262.3s\n",
            "Ep 2910: Steps=100, Avg R (100)=-400.15, Buffer=200000, Tot Steps=291000, Time=7288.3s\n",
            "Ep 2920: Steps=100, Avg R (100)=-419.10, Buffer=200000, Tot Steps=292000, Time=7314.2s\n",
            "Ep 2930: Steps=100, Avg R (100)=-434.49, Buffer=200000, Tot Steps=293000, Time=7340.3s\n",
            "Ep 2940: Steps=100, Avg R (100)=-448.21, Buffer=200000, Tot Steps=294000, Time=7366.4s\n",
            "Ep 2950: Steps=100, Avg R (100)=-461.53, Buffer=200000, Tot Steps=295000, Time=7393.0s\n",
            "Ep 2960: Steps=100, Avg R (100)=-479.67, Buffer=200000, Tot Steps=296000, Time=7419.6s\n",
            "Ep 2970: Steps=100, Avg R (100)=-473.71, Buffer=200000, Tot Steps=297000, Time=7446.4s\n",
            "Ep 2980: Steps=100, Avg R (100)=-463.03, Buffer=200000, Tot Steps=298000, Time=7473.3s\n",
            "Ep 2990: Steps=100, Avg R (100)=-456.74, Buffer=200000, Tot Steps=299000, Time=7500.9s\n",
            "Ep 3000: Steps=100, Avg R (100)=-439.22, Buffer=200000, Tot Steps=300000, Time=7528.2s\n",
            "Ep 3010: Steps=100, Avg R (100)=-422.36, Buffer=200000, Tot Steps=301000, Time=7555.2s\n",
            "Ep 3020: Steps=100, Avg R (100)=-404.88, Buffer=200000, Tot Steps=302000, Time=7582.2s\n",
            "Ep 3030: Steps=100, Avg R (100)=-391.95, Buffer=200000, Tot Steps=303000, Time=7610.1s\n",
            "Ep 3040: Steps=100, Avg R (100)=-377.23, Buffer=200000, Tot Steps=304000, Time=7637.4s\n",
            "Ep 3050: Steps=100, Avg R (100)=-367.72, Buffer=200000, Tot Steps=305000, Time=7664.8s\n",
            "Ep 3060: Steps=100, Avg R (100)=-355.56, Buffer=200000, Tot Steps=306000, Time=7692.1s\n",
            "Ep 3070: Steps=100, Avg R (100)=-366.83, Buffer=200000, Tot Steps=307000, Time=7720.3s\n",
            "Ep 3080: Steps=100, Avg R (100)=-385.12, Buffer=200000, Tot Steps=308000, Time=7747.5s\n",
            "Ep 3090: Steps=100, Avg R (100)=-396.67, Buffer=200000, Tot Steps=309000, Time=7774.7s\n",
            "Ep 3100: Steps=100, Avg R (100)=-411.09, Buffer=200000, Tot Steps=310000, Time=7802.4s\n",
            "Ep 3110: Steps=100, Avg R (100)=-431.83, Buffer=200000, Tot Steps=311000, Time=7830.4s\n",
            "Ep 3120: Steps=100, Avg R (100)=-443.42, Buffer=200000, Tot Steps=312000, Time=7857.9s\n",
            "Ep 3130: Steps=100, Avg R (100)=-457.23, Buffer=200000, Tot Steps=313000, Time=7885.5s\n",
            "Ep 3140: Steps=100, Avg R (100)=-466.66, Buffer=200000, Tot Steps=314000, Time=7913.6s\n",
            "Ep 3150: Steps=100, Avg R (100)=-473.28, Buffer=200000, Tot Steps=315000, Time=7941.0s\n",
            "Ep 3160: Steps=100, Avg R (100)=-484.97, Buffer=200000, Tot Steps=316000, Time=7968.9s\n",
            "Ep 3170: Steps=100, Avg R (100)=-488.51, Buffer=200000, Tot Steps=317000, Time=7996.6s\n",
            "Ep 3180: Steps=100, Avg R (100)=-488.44, Buffer=200000, Tot Steps=318000, Time=8024.6s\n",
            "Ep 3190: Steps=100, Avg R (100)=-485.38, Buffer=200000, Tot Steps=319000, Time=8052.0s\n",
            "Ep 3200: Steps=100, Avg R (100)=-480.91, Buffer=200000, Tot Steps=320000, Time=8079.3s\n",
            "Ep 3210: Steps=100, Avg R (100)=-464.88, Buffer=200000, Tot Steps=321000, Time=8107.2s\n",
            "Ep 3220: Steps=100, Avg R (100)=-467.66, Buffer=200000, Tot Steps=322000, Time=8133.3s\n",
            "Ep 3230: Steps=100, Avg R (100)=-468.26, Buffer=200000, Tot Steps=323000, Time=8159.4s\n",
            "Ep 3240: Steps=100, Avg R (100)=-482.22, Buffer=200000, Tot Steps=324000, Time=8185.8s\n",
            "Ep 3250: Steps=100, Avg R (100)=-475.24, Buffer=200000, Tot Steps=325000, Time=8212.0s\n",
            "Ep 3260: Steps=100, Avg R (100)=-463.13, Buffer=200000, Tot Steps=326000, Time=8238.1s\n",
            "Ep 3270: Steps=100, Avg R (100)=-448.15, Buffer=200000, Tot Steps=327000, Time=8265.2s\n",
            "Ep 3280: Steps=100, Avg R (100)=-445.98, Buffer=200000, Tot Steps=328000, Time=8291.5s\n",
            "Ep 3290: Steps=100, Avg R (100)=-433.51, Buffer=200000, Tot Steps=329000, Time=8317.3s\n",
            "Ep 3300: Steps=100, Avg R (100)=-432.19, Buffer=200000, Tot Steps=330000, Time=8343.3s\n",
            "Ep 3310: Steps=100, Avg R (100)=-425.64, Buffer=200000, Tot Steps=331000, Time=8369.3s\n",
            "Ep 3320: Steps=100, Avg R (100)=-407.84, Buffer=200000, Tot Steps=332000, Time=8395.6s\n",
            "Ep 3330: Steps=100, Avg R (100)=-404.59, Buffer=200000, Tot Steps=333000, Time=8422.1s\n",
            "Ep 3340: Steps=100, Avg R (100)=-400.02, Buffer=200000, Tot Steps=334000, Time=8448.6s\n",
            "Ep 3350: Steps=100, Avg R (100)=-392.85, Buffer=200000, Tot Steps=335000, Time=8474.8s\n",
            "Ep 3360: Steps=100, Avg R (100)=-400.91, Buffer=200000, Tot Steps=336000, Time=8500.7s\n",
            "Ep 3370: Steps=100, Avg R (100)=-400.17, Buffer=200000, Tot Steps=337000, Time=8526.8s\n",
            "Ep 3380: Steps=100, Avg R (100)=-386.04, Buffer=200000, Tot Steps=338000, Time=8552.6s\n",
            "Ep 3390: Steps=100, Avg R (100)=-383.21, Buffer=200000, Tot Steps=339000, Time=8578.5s\n",
            "Ep 3400: Steps=100, Avg R (100)=-384.66, Buffer=200000, Tot Steps=340000, Time=8605.0s\n",
            "Ep 3410: Steps=100, Avg R (100)=-383.66, Buffer=200000, Tot Steps=341000, Time=8630.8s\n",
            "Ep 3420: Steps=100, Avg R (100)=-386.61, Buffer=200000, Tot Steps=342000, Time=8656.6s\n",
            "Ep 3430: Steps=100, Avg R (100)=-374.39, Buffer=200000, Tot Steps=343000, Time=8682.5s\n",
            "Ep 3440: Steps=100, Avg R (100)=-358.65, Buffer=200000, Tot Steps=344000, Time=8708.5s\n",
            "Ep 3450: Steps=100, Avg R (100)=-363.20, Buffer=200000, Tot Steps=345000, Time=8734.5s\n",
            "Ep 3460: Steps=100, Avg R (100)=-361.13, Buffer=200000, Tot Steps=346000, Time=8760.8s\n",
            "Ep 3470: Steps=100, Avg R (100)=-370.93, Buffer=200000, Tot Steps=347000, Time=8787.3s\n",
            "Ep 3480: Steps=100, Avg R (100)=-382.70, Buffer=200000, Tot Steps=348000, Time=8813.4s\n",
            "Ep 3490: Steps=100, Avg R (100)=-388.98, Buffer=200000, Tot Steps=349000, Time=8839.2s\n",
            "Ep 3500: Steps=100, Avg R (100)=-388.27, Buffer=200000, Tot Steps=350000, Time=8865.0s\n",
            "Ep 3510: Steps=100, Avg R (100)=-389.20, Buffer=200000, Tot Steps=351000, Time=8890.8s\n",
            "Ep 3520: Steps=100, Avg R (100)=-404.73, Buffer=200000, Tot Steps=352000, Time=8916.7s\n",
            "Ep 3530: Steps=100, Avg R (100)=-415.93, Buffer=200000, Tot Steps=353000, Time=8943.3s\n",
            "Ep 3540: Steps=100, Avg R (100)=-429.56, Buffer=200000, Tot Steps=354000, Time=8969.5s\n",
            "Ep 3550: Steps=100, Avg R (100)=-451.11, Buffer=200000, Tot Steps=355000, Time=8995.4s\n",
            "Ep 3560: Steps=100, Avg R (100)=-454.62, Buffer=200000, Tot Steps=356000, Time=9021.3s\n",
            "Ep 3570: Steps=100, Avg R (100)=-449.47, Buffer=200000, Tot Steps=357000, Time=9046.9s\n",
            "Ep 3580: Steps=100, Avg R (100)=-445.98, Buffer=200000, Tot Steps=358000, Time=9072.6s\n",
            "Ep 3590: Steps=100, Avg R (100)=-451.86, Buffer=200000, Tot Steps=359000, Time=9098.4s\n",
            "Ep 3600: Steps=100, Avg R (100)=-454.59, Buffer=200000, Tot Steps=360000, Time=9124.8s\n",
            "Ep 3610: Steps=100, Avg R (100)=-456.82, Buffer=200000, Tot Steps=361000, Time=9150.6s\n",
            "Ep 3620: Steps=100, Avg R (100)=-447.72, Buffer=200000, Tot Steps=362000, Time=9176.3s\n",
            "Ep 3630: Steps=100, Avg R (100)=-446.07, Buffer=200000, Tot Steps=363000, Time=9202.3s\n",
            "Ep 3640: Steps=100, Avg R (100)=-433.22, Buffer=200000, Tot Steps=364000, Time=9228.1s\n",
            "Ep 3650: Steps=100, Avg R (100)=-417.10, Buffer=200000, Tot Steps=365000, Time=9253.7s\n",
            "Ep 3660: Steps=100, Avg R (100)=-402.36, Buffer=200000, Tot Steps=366000, Time=9279.3s\n",
            "Ep 3670: Steps=100, Avg R (100)=-404.69, Buffer=200000, Tot Steps=367000, Time=9305.5s\n",
            "Ep 3680: Steps=100, Avg R (100)=-410.95, Buffer=200000, Tot Steps=368000, Time=9333.1s\n",
            "Ep 3690: Steps=100, Avg R (100)=-410.51, Buffer=200000, Tot Steps=369000, Time=9360.3s\n",
            "Ep 3700: Steps=100, Avg R (100)=-426.08, Buffer=200000, Tot Steps=370000, Time=9386.9s\n",
            "Ep 3710: Steps=100, Avg R (100)=-430.77, Buffer=200000, Tot Steps=371000, Time=9413.9s\n",
            "Ep 3720: Steps=100, Avg R (100)=-441.35, Buffer=200000, Tot Steps=372000, Time=9441.4s\n",
            "Ep 3730: Steps=100, Avg R (100)=-447.97, Buffer=200000, Tot Steps=373000, Time=9467.8s\n",
            "Ep 3740: Steps=100, Avg R (100)=-460.32, Buffer=200000, Tot Steps=374000, Time=9493.9s\n",
            "Ep 3750: Steps=100, Avg R (100)=-482.38, Buffer=200000, Tot Steps=375000, Time=9520.2s\n",
            "Ep 3760: Steps=100, Avg R (100)=-488.22, Buffer=200000, Tot Steps=376000, Time=9546.2s\n",
            "Ep 3770: Steps=100, Avg R (100)=-486.65, Buffer=200000, Tot Steps=377000, Time=9572.7s\n",
            "Ep 3780: Steps=100, Avg R (100)=-485.05, Buffer=200000, Tot Steps=378000, Time=9599.0s\n",
            "Ep 3790: Steps=100, Avg R (100)=-492.51, Buffer=200000, Tot Steps=379000, Time=9625.1s\n",
            "Ep 3800: Steps=100, Avg R (100)=-479.59, Buffer=200000, Tot Steps=380000, Time=9651.1s\n",
            "Ep 3810: Steps=100, Avg R (100)=-478.07, Buffer=200000, Tot Steps=381000, Time=9677.2s\n",
            "Ep 3820: Steps=100, Avg R (100)=-481.50, Buffer=200000, Tot Steps=382000, Time=9703.1s\n",
            "Ep 3830: Steps=100, Avg R (100)=-480.96, Buffer=200000, Tot Steps=383000, Time=9729.2s\n",
            "Ep 3840: Steps=100, Avg R (100)=-491.32, Buffer=200000, Tot Steps=384000, Time=9755.8s\n",
            "Ep 3850: Steps=100, Avg R (100)=-474.74, Buffer=200000, Tot Steps=385000, Time=9781.9s\n",
            "Ep 3860: Steps=100, Avg R (100)=-491.01, Buffer=200000, Tot Steps=386000, Time=9807.8s\n",
            "Ep 3870: Steps=100, Avg R (100)=-515.75, Buffer=200000, Tot Steps=387000, Time=9834.0s\n",
            "Ep 3880: Steps=100, Avg R (100)=-518.79, Buffer=200000, Tot Steps=388000, Time=9860.0s\n",
            "Ep 3890: Steps=100, Avg R (100)=-515.22, Buffer=200000, Tot Steps=389000, Time=9886.0s\n",
            "Ep 3900: Steps=100, Avg R (100)=-523.50, Buffer=200000, Tot Steps=390000, Time=9912.0s\n",
            "Ep 3910: Steps=100, Avg R (100)=-529.19, Buffer=200000, Tot Steps=391000, Time=9938.6s\n",
            "Ep 3920: Steps=100, Avg R (100)=-520.39, Buffer=200000, Tot Steps=392000, Time=9964.7s\n",
            "Ep 3930: Steps=100, Avg R (100)=-521.73, Buffer=200000, Tot Steps=393000, Time=9990.8s\n",
            "Ep 3940: Steps=100, Avg R (100)=-527.26, Buffer=200000, Tot Steps=394000, Time=10016.8s\n",
            "Ep 3950: Steps=100, Avg R (100)=-532.74, Buffer=200000, Tot Steps=395000, Time=10042.9s\n",
            "Ep 3960: Steps=100, Avg R (100)=-533.72, Buffer=200000, Tot Steps=396000, Time=10069.0s\n",
            "Ep 3970: Steps=100, Avg R (100)=-530.62, Buffer=200000, Tot Steps=397000, Time=10095.4s\n",
            "Ep 3980: Steps=100, Avg R (100)=-534.43, Buffer=200000, Tot Steps=398000, Time=10121.7s\n",
            "Ep 3990: Steps=100, Avg R (100)=-530.82, Buffer=200000, Tot Steps=399000, Time=10147.8s\n",
            "Ep 4000: Steps=100, Avg R (100)=-542.45, Buffer=200000, Tot Steps=400000, Time=10173.7s\n",
            "Ep 4010: Steps=100, Avg R (100)=-531.68, Buffer=200000, Tot Steps=401000, Time=10199.6s\n",
            "Ep 4020: Steps=100, Avg R (100)=-534.77, Buffer=200000, Tot Steps=402000, Time=10225.5s\n",
            "Ep 4030: Steps=100, Avg R (100)=-540.71, Buffer=200000, Tot Steps=403000, Time=10251.3s\n",
            "Ep 4040: Steps=100, Avg R (100)=-533.78, Buffer=200000, Tot Steps=404000, Time=10278.2s\n",
            "Ep 4050: Steps=100, Avg R (100)=-536.02, Buffer=200000, Tot Steps=405000, Time=10304.3s\n",
            "Ep 4060: Steps=100, Avg R (100)=-546.43, Buffer=200000, Tot Steps=406000, Time=10330.3s\n",
            "Ep 4070: Steps=100, Avg R (100)=-530.76, Buffer=200000, Tot Steps=407000, Time=10356.2s\n",
            "Ep 4080: Steps=100, Avg R (100)=-523.28, Buffer=200000, Tot Steps=408000, Time=10382.0s\n",
            "Ep 4090: Steps=100, Avg R (100)=-527.30, Buffer=200000, Tot Steps=409000, Time=10407.6s\n",
            "Ep 4100: Steps=100, Avg R (100)=-510.28, Buffer=200000, Tot Steps=410000, Time=10433.2s\n",
            "Ep 4110: Steps=100, Avg R (100)=-532.45, Buffer=200000, Tot Steps=411000, Time=10459.5s\n",
            "Ep 4120: Steps=100, Avg R (100)=-536.74, Buffer=200000, Tot Steps=412000, Time=10485.5s\n",
            "Ep 4130: Steps=100, Avg R (100)=-528.53, Buffer=200000, Tot Steps=413000, Time=10511.7s\n",
            "Ep 4140: Steps=100, Avg R (100)=-518.23, Buffer=200000, Tot Steps=414000, Time=10537.6s\n",
            "Ep 4150: Steps=100, Avg R (100)=-510.84, Buffer=200000, Tot Steps=415000, Time=10563.5s\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# --- Installation ---\n",
        "# !pip install pettingzoo[mpe] tensorflow matplotlib imageio ipython pygame\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import io\n",
        "import base64\n",
        "from collections import deque, defaultdict\n",
        "import warnings\n",
        "\n",
        "# Suppress DeprecationWarnings from PettingZoo MPE C API (optional)\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from pettingzoo.mpe import simple_spread_v3\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML, display\n",
        "import imageio\n",
        "import pygame\n",
        "\n",
        "# Check for GPU availability\n",
        "if tf.config.list_physical_devices('GPU'):\n",
        "    print(\"GPU is available and will be used.\")\n",
        "    physical_devices = tf.config.list_physical_devices('GPU')\n",
        "    try:\n",
        "        for gpu in physical_devices:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(\"Enabled GPU memory growth.\")\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n",
        "    device = '/GPU:0'\n",
        "else:\n",
        "    print(\"GPU is not available. CPU will be used.\")\n",
        "    device = '/CPU:0'\n",
        "\n",
        "# --- TensorBoard Setup ---\n",
        "log_dir = \"masac_v2_tensorflow_logs_simple_spread_fixed_alpha_beta\"\n",
        "if not os.path.exists(log_dir):\n",
        "    os.makedirs(log_dir)\n",
        "summary_writer = tf.summary.create_file_writer(log_dir)\n",
        "print(f\"TensorBoard logs will be saved to: {log_dir}\")\n",
        "\n",
        "# --- Environment Setup ---\n",
        "def make_env(env_name=\"simple_spread_v3\", continuous_actions=True, max_cycles=25, render_mode=None):\n",
        "    \"\"\"Creates the PettingZoo environment.\"\"\"\n",
        "    if env_name == \"simple_spread_v3\":\n",
        "        env = simple_spread_v3.parallel_env(\n",
        "            N=3,\n",
        "            local_ratio=0.5,\n",
        "            max_cycles=max_cycles,\n",
        "            continuous_actions=continuous_actions,\n",
        "            render_mode=render_mode\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported MPE environment name: {env_name}\")\n",
        "    try:\n",
        "        _ = env.reset()\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: env.reset() raised an exception: {e}. Attempting reset without options.\")\n",
        "        _ = env.reset()\n",
        "    return env\n",
        "\n",
        "# --- Replay Buffer ---\n",
        "class MultiAgentReplayBuffer:\n",
        "    def __init__(self, capacity, agent_ids, obs_dims, action_dims):\n",
        "        self.capacity = capacity\n",
        "        self.agent_ids = list(agent_ids)\n",
        "        self.num_agents = len(agent_ids)\n",
        "        self.obs_dims = obs_dims\n",
        "        self.action_dims = action_dims\n",
        "        self.total_obs_dim = sum(self.obs_dims[agent_id] for agent_id in self.agent_ids)\n",
        "        self.total_action_dim = sum(self.action_dims[agent_id] for agent_id in self.agent_ids)\n",
        "        self.obs_buffer = np.zeros((capacity, self.total_obs_dim), dtype=np.float32)\n",
        "        self.action_buffer = np.zeros((capacity, self.total_action_dim), dtype=np.float32)\n",
        "        self.reward_buffer = np.zeros((capacity, self.num_agents), dtype=np.float32)\n",
        "        self.next_obs_buffer = np.zeros((capacity, self.total_obs_dim), dtype=np.float32)\n",
        "        self.done_buffer = np.zeros((capacity, self.num_agents), dtype=np.float32)\n",
        "        self.ptr = 0\n",
        "        self.size = 0\n",
        "\n",
        "    def _dict_to_concatenated(self, data_dict, dim_map):\n",
        "        \"\"\"Concatenates data from agent dicts into a single array based on self.agent_ids order.\"\"\"\n",
        "        default_shape_val = np.zeros(dim_map[self.agent_ids[0]], dtype=np.float32)\n",
        "        concatenated_list = []\n",
        "        for agent_id in self.agent_ids:\n",
        "            agent_data = data_dict.get(agent_id, default_shape_val)\n",
        "            if not isinstance(agent_data, np.ndarray):\n",
        "                try:\n",
        "                    agent_data = np.array(agent_data, dtype=np.float32)\n",
        "                except Exception as e:\n",
        "                    agent_data = default_shape_val\n",
        "            concatenated_list.append(agent_data.flatten())\n",
        "        if not concatenated_list:\n",
        "            return np.zeros(sum(dim_map.values()), dtype=np.float32)\n",
        "        try:\n",
        "            result = np.concatenate(concatenated_list, axis=0).astype(np.float32)\n",
        "            return result\n",
        "        except ValueError as e:\n",
        "            print(f\"Error during final concatenation: {e}\")\n",
        "            print(\"Shapes in list for concatenation:\")\n",
        "            for i, arr in enumerate(concatenated_list):\n",
        "                print(f\"  Item {i}: {arr.shape}\")\n",
        "            raise e\n",
        "\n",
        "    def push(self, obs_dict, action_dict, reward_dict, next_obs_dict, done_dict):\n",
        "        \"\"\"Stores a transition for all agents.\"\"\"\n",
        "        try:\n",
        "            if not all(agent_id in obs_dict and obs_dict[agent_id] is not None for agent_id in self.agent_ids) or \\\n",
        "               not all(agent_id in next_obs_dict and next_obs_dict[agent_id] is not None for agent_id in self.agent_ids):\n",
        "                return\n",
        "            obs_concat = self._dict_to_concatenated(obs_dict, self.obs_dims)\n",
        "            next_obs_concat = self._dict_to_concatenated(next_obs_dict, self.obs_dims)\n",
        "            action_concat = self._dict_to_concatenated(action_dict, self.action_dims)\n",
        "            rewards = np.array([reward_dict.get(agent_id, 0.0) for agent_id in self.agent_ids], dtype=np.float32)\n",
        "            dones = np.array([done_dict.get(agent_id, False) for agent_id in self.agent_ids], dtype=np.float32)\n",
        "            expected_obs_shape = (self.total_obs_dim,)\n",
        "            expected_action_shape = (self.total_action_dim,)\n",
        "            if obs_concat.shape != expected_obs_shape or \\\n",
        "               next_obs_concat.shape != expected_obs_shape or \\\n",
        "               action_concat.shape != expected_action_shape:\n",
        "                return\n",
        "            self.obs_buffer[self.ptr] = obs_concat\n",
        "            self.action_buffer[self.ptr] = action_concat\n",
        "            self.reward_buffer[self.ptr] = rewards\n",
        "            self.next_obs_buffer[self.ptr] = next_obs_concat\n",
        "            self.done_buffer[self.ptr] = dones\n",
        "            self.ptr = (self.ptr + 1) % self.capacity\n",
        "            self.size = min(self.size + 1, self.capacity)\n",
        "        except KeyError as e:\n",
        "            pass\n",
        "        except ValueError as e:\n",
        "            pass\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"Samples a batch of transitions.\"\"\"\n",
        "        actual_batch_size = min(self.size, batch_size)\n",
        "        if actual_batch_size == 0:\n",
        "            return None\n",
        "        indices = np.random.choice(self.size, actual_batch_size, replace=False)\n",
        "        return (\n",
        "            self.obs_buffer[indices],\n",
        "            self.action_buffer[indices],\n",
        "            self.reward_buffer[indices],\n",
        "            self.next_obs_buffer[indices],\n",
        "            self.done_buffer[indices]\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Returns the current size of the buffer.\"\"\"\n",
        "        return self.size\n",
        "\n",
        "# --- Network Architectures ---\n",
        "def create_actor_network(state_dim, action_dim, hidden_units):\n",
        "    inputs = tf.keras.layers.Input(shape=(state_dim,))\n",
        "    x = tf.keras.layers.Dense(hidden_units, activation='relu', kernel_initializer=tf.keras.initializers.HeUniform())(inputs)\n",
        "    x = tf.keras.layers.Dense(hidden_units, activation='relu', kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
        "    mu = tf.keras.layers.Dense(action_dim, kernel_initializer=tf.keras.initializers.GlorotUniform())(x)\n",
        "    log_std = tf.keras.layers.Dense(action_dim, kernel_initializer=tf.keras.initializers.GlorotUniform())(x)\n",
        "    log_std = tf.keras.layers.Lambda(lambda t: tf.clip_by_value(t, -20, 2))(log_std)\n",
        "    return tf.keras.Model(inputs=inputs, outputs=[mu, log_std])\n",
        "\n",
        "def create_critic_network(total_obs_dim, total_action_dim, hidden_units):\n",
        "    obs_input = tf.keras.layers.Input(shape=(total_obs_dim,))\n",
        "    action_input = tf.keras.layers.Input(shape=(total_action_dim,))\n",
        "    concat = tf.keras.layers.Concatenate()([obs_input, action_input])\n",
        "    x = tf.keras.layers.Dense(hidden_units, activation='relu', kernel_initializer=tf.keras.initializers.HeUniform())(concat)\n",
        "    x = tf.keras.layers.Dense(hidden_units, activation='relu', kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
        "    q_value = tf.keras.layers.Dense(1, kernel_initializer=tf.keras.initializers.GlorotUniform())(x)\n",
        "    return tf.keras.Model(inputs=[obs_input, action_input], outputs=q_value)\n",
        "\n",
        "# --- Log Probability Helper ---\n",
        "@tf.function(reduce_retracing=True)\n",
        "def gaussian_log_prob(x, mu, log_std):\n",
        "    \"\"\"Calculates log probability log P(x|N(mu, std)) for diagonal Gaussian.\"\"\"\n",
        "    std = tf.exp(log_std)\n",
        "    std_safe = std + 1e-7\n",
        "    var_safe = tf.square(std_safe)\n",
        "    log_unnormalized = -0.5 * tf.square(x - mu) / var_safe\n",
        "    var_safe_clipped = tf.maximum(var_safe, 1e-14)\n",
        "    log_normalization = 0.5 * tf.math.log(2.0 * np.pi * var_safe_clipped)\n",
        "    log_prob_per_dim = log_unnormalized - log_normalization\n",
        "    return tf.reduce_sum(log_prob_per_dim, axis=1, keepdims=True)\n",
        "\n",
        "# --- Multi-Agent SAC v2 (MA-SAC) Agent ---\n",
        "class MASACAgentV2:\n",
        "    def __init__(self, env, agent_ids, obs_dims, action_dims, action_spaces,\n",
        "                 hidden_units=256, actor_lr=3e-4, critic_lr=3e-4,\n",
        "                 tau=0.005, gamma=0.99, buffer_capacity=1000000, batch_size=256,\n",
        "                 initial_alpha=0.2, initial_beta=0.1,\n",
        "                 target_entropy_scale=1.0, target_ce_scale=0.1,\n",
        "                 gradient_clip_norm=1.0):\n",
        "        self.agent_ids = list(agent_ids)\n",
        "        self.num_agents = len(agent_ids)\n",
        "        self.obs_dims = obs_dims\n",
        "        self.action_dims = action_dims\n",
        "        self.action_spaces = action_spaces\n",
        "        self.tau = tau\n",
        "        self.gamma = gamma\n",
        "        self.batch_size = batch_size\n",
        "        self.gradient_clip_norm = gradient_clip_norm\n",
        "\n",
        "        self.total_obs_dim = sum(self.obs_dims[agent_id] for agent_id in self.agent_ids)\n",
        "        self.total_action_dim = sum(self.action_dims[agent_id] for agent_id in self.agent_ids)\n",
        "\n",
        "        # --- Actors (Decentralized) ---\n",
        "        self.actors = {}\n",
        "        self.actor_optimizers = {}\n",
        "        self.target_entropies = {}\n",
        "        self.target_cross_entropies = {}\n",
        "        for agent_id in self.agent_ids:\n",
        "            action_dim = self.action_dims[agent_id]\n",
        "            self.actors[agent_id] = create_actor_network(obs_dims[agent_id], action_dim, hidden_units)\n",
        "            self.actors[agent_id]._name = f\"actor_{agent_id}\"\n",
        "            self.actor_optimizers[agent_id] = tf.keras.optimizers.Adam(learning_rate=actor_lr)\n",
        "            self.target_entropies[agent_id] = tf.constant(-float(action_dim) * target_entropy_scale, dtype=tf.float32)\n",
        "            self.target_cross_entropies[agent_id] = tf.constant(-float(action_dim) * target_ce_scale, dtype=tf.float32)\n",
        "\n",
        "        # --- Critic (Centralized) ---\n",
        "        self.critic_1 = create_critic_network(self.total_obs_dim, self.total_action_dim, hidden_units)\n",
        "        self.critic_2 = create_critic_network(self.total_obs_dim, self.total_action_dim, hidden_units)\n",
        "        self.target_critic_1 = create_critic_network(self.total_obs_dim, self.total_action_dim, hidden_units)\n",
        "        self.target_critic_2 = create_critic_network(self.total_obs_dim, self.total_action_dim, hidden_units)\n",
        "        self.target_critic_1.set_weights(self.critic_1.get_weights())\n",
        "        self.target_critic_2.set_weights(self.critic_2.get_weights())\n",
        "        self.critic_1_optimizer = tf.keras.optimizers.Adam(learning_rate=critic_lr)\n",
        "        self.critic_2_optimizer = tf.keras.optimizers.Adam(learning_rate=critic_lr)\n",
        "\n",
        "        # --- Fixed Alpha and Beta ---\n",
        "        self.alpha = tf.constant(initial_alpha, dtype=tf.float32, name=\"alpha\")\n",
        "        self.beta = tf.constant(initial_beta, dtype=tf.float32, name=\"beta\")\n",
        "\n",
        "        # --- Replay Buffer ---\n",
        "        self.replay_buffer = MultiAgentReplayBuffer(buffer_capacity, agent_ids, obs_dims, action_dims)\n",
        "\n",
        "        # --- Helper for splitting concatenated data ---\n",
        "        self.agent_obs_indices = {}\n",
        "        self.agent_action_indices = {}\n",
        "        start_obs = 0\n",
        "        start_action = 0\n",
        "        for i, agent_id in enumerate(self.agent_ids):\n",
        "            end_obs = start_obs + self.obs_dims[agent_id]\n",
        "            end_action = start_action + self.action_dims[agent_id]\n",
        "            self.agent_obs_indices[agent_id] = (start_obs, end_obs)\n",
        "            self.agent_action_indices[agent_id] = (start_action, end_action)\n",
        "            start_obs = end_obs\n",
        "            start_action = end_action\n",
        "\n",
        "    def _get_agent_obs(self, concatenated_obs, agent_id):\n",
        "        start, end = self.agent_obs_indices[agent_id]\n",
        "        return concatenated_obs[:, start:end]\n",
        "\n",
        "    def _get_agent_action(self, concatenated_action, agent_id):\n",
        "        start, end = self.agent_action_indices[agent_id]\n",
        "        return concatenated_action[:, start:end]\n",
        "\n",
        "    @tf.function\n",
        "    def _get_action_from_actor(self, actor_model, state_tensor, evaluate):\n",
        "        \"\"\"Helper function to get action from actor model.\"\"\"\n",
        "        mu, log_std = actor_model(state_tensor, training=not evaluate)\n",
        "        tf.debugging.check_numerics(mu, f\"Actor mu output for {actor_model.name}\")\n",
        "        tf.debugging.check_numerics(log_std, f\"Actor log_std output for {actor_model.name}\")\n",
        "        if evaluate:\n",
        "            raw_action = mu\n",
        "        else:\n",
        "            std = tf.exp(log_std)\n",
        "            epsilon = tf.random.normal(shape=tf.shape(mu))\n",
        "            raw_action = mu + std * epsilon\n",
        "        tf.debugging.check_numerics(raw_action, f\"Raw action before tanh for {actor_model.name}\")\n",
        "        action_tanh = tf.tanh(raw_action)\n",
        "        tf.debugging.check_numerics(action_tanh, f\"Action after tanh for {actor_model.name}\")\n",
        "        return action_tanh\n",
        "\n",
        "    def get_actions(self, obs_dict, evaluate=False):\n",
        "        actions_dict = {}\n",
        "        with tf.device(device):\n",
        "            for i, agent_id in enumerate(self.agent_ids):\n",
        "                if agent_id not in obs_dict or obs_dict[agent_id] is None:\n",
        "                    actions_dict[agent_id] = np.zeros(self.action_dims[agent_id], dtype=np.float32)\n",
        "                    continue\n",
        "                state = obs_dict[agent_id][None, ...]\n",
        "                try:\n",
        "                    state_tensor = tf.convert_to_tensor(state, dtype=tf.float32)\n",
        "                    if np.any(np.isnan(state)):\n",
        "                        print(f\"Warning: NaN detected in input observation for agent {agent_id}\")\n",
        "                        actions_dict[agent_id] = np.zeros(self.action_dims[agent_id], dtype=np.float32)\n",
        "                        continue\n",
        "                except Exception as e:\n",
        "                    actions_dict[agent_id] = np.zeros(self.action_dims[agent_id], dtype=np.float32)\n",
        "                    continue\n",
        "                actor_model = self.actors[agent_id]\n",
        "                try:\n",
        "                    action_tanh = self._get_action_from_actor(actor_model, state_tensor, evaluate)\n",
        "                    action_tanh_np = action_tanh.numpy()[0]\n",
        "                    if np.any(np.isnan(action_tanh_np)):\n",
        "                        print(f\"Warning: NaN detected in action_tanh_np for agent {agent_id}. Replacing with zeros.\")\n",
        "                        actions_dict[agent_id] = np.zeros(self.action_dims[agent_id], dtype=np.float32)\n",
        "                        continue\n",
        "                except tf.errors.InvalidArgumentError as e:\n",
        "                    print(f\"!!! Numerical stability error during action selection for agent {agent_id}: {e} !!!\")\n",
        "                    actions_dict[agent_id] = np.zeros(self.action_dims[agent_id], dtype=np.float32)\n",
        "                    continue\n",
        "                action_space = self.action_spaces[agent_id]\n",
        "                action_low = action_space.low.astype(np.float32)\n",
        "                action_high = action_space.high.astype(np.float32)\n",
        "                scaled_action = action_low + (action_tanh_np + 1.0) * 0.5 * (action_high - action_low)\n",
        "                scaled_action = np.clip(scaled_action, action_low, action_high)\n",
        "                if np.any(np.isnan(scaled_action)):\n",
        "                    print(f\"Warning: NaN detected in final scaled_action for agent {agent_id}. Replacing with zeros.\")\n",
        "                    actions_dict[agent_id] = np.zeros(self.action_dims[agent_id], dtype=np.float32)\n",
        "                else:\n",
        "                    actions_dict[agent_id] = scaled_action\n",
        "        return actions_dict\n",
        "\n",
        "    @tf.function(reduce_retracing=True)\n",
        "    def _get_sampled_actions_and_log_probs(self, actor_model, agent_obs):\n",
        "        \"\"\"Samples an action and calculates its log probability.\"\"\"\n",
        "        mu, log_std = actor_model(agent_obs, training=True)\n",
        "        std = tf.exp(log_std)\n",
        "        epsilon = tf.random.normal(shape=tf.shape(mu))\n",
        "        action_raw = mu + std * epsilon\n",
        "        action_tanh = tf.tanh(action_raw)\n",
        "        log_prob_raw = gaussian_log_prob(action_raw, mu, log_std)\n",
        "        log_prob_tanh = log_prob_raw - tf.reduce_sum(tf.math.log(1.0 - tf.square(action_tanh) + 1e-7), axis=1, keepdims=True)\n",
        "        tf.debugging.check_numerics(action_tanh, \"Sampled action tanh\")\n",
        "        tf.debugging.check_numerics(log_prob_tanh, \"Sampled action log_prob\")\n",
        "        return action_tanh, log_prob_tanh\n",
        "\n",
        "    @tf.function(reduce_retracing=True)\n",
        "    def _get_log_prob_under_policy(self, policy_actor_model, eval_agent_obs, action_tanh):\n",
        "        \"\"\"Calculates log prob of action_tanh under policy_actor_model given eval_agent_obs.\"\"\"\n",
        "        mu, log_std = policy_actor_model(eval_agent_obs, training=True)\n",
        "        action_tanh_clipped = tf.clip_by_value(action_tanh, -1.0 + 1e-7, 1.0 - 1e-7)\n",
        "        action_raw = tf.atanh(action_tanh_clipped)\n",
        "        log_prob_raw = gaussian_log_prob(action_raw, mu, log_std)\n",
        "        log_prob_tanh = log_prob_raw - tf.reduce_sum(tf.math.log(1.0 - tf.square(action_tanh) + 1e-7), axis=1, keepdims=True)\n",
        "        tf.debugging.check_numerics(log_prob_tanh, \"Cross-policy log_prob\")\n",
        "        return log_prob_tanh\n",
        "\n",
        "    @tf.function\n",
        "    def _update_networks(self, batch_obs, batch_actions, batch_rewards, batch_next_obs, batch_dones):\n",
        "        \"\"\"Performs one update step for critics and actors.\"\"\"\n",
        "        # ---------------------- Critic Update ----------------------\n",
        "        with tf.GradientTape(persistent=True) as critic_tape:\n",
        "            next_actions_list = []\n",
        "            for i, agent_id in enumerate(self.agent_ids):\n",
        "                agent_next_obs = self._get_agent_obs(batch_next_obs, agent_id)\n",
        "                action_tanh_next, _ = self._get_sampled_actions_and_log_probs(\n",
        "                    self.actors[agent_id], agent_next_obs\n",
        "                )\n",
        "                next_actions_list.append(action_tanh_next)\n",
        "            next_actions_concat = tf.concat(next_actions_list, axis=1)\n",
        "            tf.debugging.check_numerics(next_actions_concat, \"Critic next actions\")\n",
        "            target_q1 = self.target_critic_1([batch_next_obs, next_actions_concat], training=False)\n",
        "            target_q2 = self.target_critic_2([batch_next_obs, next_actions_concat], training=False)\n",
        "            target_q = tf.minimum(target_q1, target_q2)\n",
        "            tf.debugging.check_numerics(target_q, \"Critic target Q\")\n",
        "            mean_reward = tf.reduce_mean(batch_rewards, axis=1, keepdims=True)\n",
        "            shared_done = tf.reduce_max(batch_dones, axis=1, keepdims=True)\n",
        "            target_y = mean_reward + self.gamma * (1.0 - shared_done) * tf.stop_gradient(target_q)\n",
        "            tf.debugging.check_numerics(target_y, \"Critic target y\")\n",
        "            q1 = self.critic_1([batch_obs, batch_actions], training=True)\n",
        "            q2 = self.critic_2([batch_obs, batch_actions], training=True)\n",
        "            tf.debugging.check_numerics(q1, \"Critic Q1\")\n",
        "            tf.debugging.check_numerics(q2, \"Critic Q2\")\n",
        "            critic_1_loss = tf.reduce_mean(tf.square(q1 - target_y))\n",
        "            critic_2_loss = tf.reduce_mean(tf.square(q2 - target_y))\n",
        "            critic_loss = critic_1_loss + critic_2_loss\n",
        "            tf.debugging.check_numerics(critic_loss, \"Critic total loss\")\n",
        "        critic_1_grads = critic_tape.gradient(critic_1_loss, self.critic_1.trainable_variables)\n",
        "        critic_2_grads = critic_tape.gradient(critic_2_loss, self.critic_2.trainable_variables)\n",
        "        del critic_tape\n",
        "        if self.gradient_clip_norm is not None:\n",
        "            if critic_1_grads is not None and all(g is not None for g in critic_1_grads):\n",
        "                critic_1_grads, _ = tf.clip_by_global_norm(critic_1_grads, self.gradient_clip_norm)\n",
        "            if critic_2_grads is not None and all(g is not None for g in critic_2_grads):\n",
        "                critic_2_grads, _ = tf.clip_by_global_norm(critic_2_grads, self.gradient_clip_norm)\n",
        "        if critic_1_grads is not None and all(g is not None for g in critic_1_grads):\n",
        "            self.critic_1_optimizer.apply_gradients(zip(critic_1_grads, self.critic_1.trainable_variables))\n",
        "        if critic_2_grads is not None and all(g is not None for g in critic_2_grads):\n",
        "            self.critic_2_optimizer.apply_gradients(zip(critic_2_grads, self.critic_2.trainable_variables))\n",
        "\n",
        "        # ---------------------- Actor Update ----------------------\n",
        "        actor_loss_list = []\n",
        "        with tf.GradientTape(persistent=True) as actor_tape:\n",
        "            agent_actions_tanh_list = []\n",
        "            agent_log_probs_tanh_list = []\n",
        "            for i, agent_id in enumerate(self.agent_ids):\n",
        "                agent_obs = self._get_agent_obs(batch_obs, agent_id)\n",
        "                action_tanh, log_prob_tanh = self._get_sampled_actions_and_log_probs(\n",
        "                    self.actors[agent_id], agent_obs\n",
        "                )\n",
        "                agent_actions_tanh_list.append(action_tanh)\n",
        "                agent_log_probs_tanh_list.append(log_prob_tanh)\n",
        "            current_actions_concat = tf.concat(agent_actions_tanh_list, axis=1)\n",
        "            q1_new = self.critic_1([batch_obs, current_actions_concat], training=True)\n",
        "            q_new = q1_new\n",
        "            tf.debugging.check_numerics(q_new, \"Actor update Q_new\")\n",
        "            for i, agent_id_i in enumerate(self.agent_ids):\n",
        "                log_prob_i = agent_log_probs_tanh_list[i]\n",
        "                action_i = agent_actions_tanh_list[i]\n",
        "                obs_i = self._get_agent_obs(batch_obs, agent_id_i)\n",
        "                log_prob_j_sum = tf.zeros_like(log_prob_i)\n",
        "                num_others = 0\n",
        "                for j, agent_id_j in enumerate(self.agent_ids):\n",
        "                    if i == j:\n",
        "                        continue\n",
        "                    log_prob_j_given_i = self._get_log_prob_under_policy(\n",
        "                        policy_actor_model=self.actors[agent_id_j],\n",
        "                        eval_agent_obs=obs_i,\n",
        "                        action_tanh=action_i\n",
        "                    )\n",
        "                    log_prob_j_sum += log_prob_j_given_i\n",
        "                    num_others += 1\n",
        "                tf.debugging.check_numerics(log_prob_i, f\"Actor log_prob_i {i}\")\n",
        "                tf.debugging.check_numerics(log_prob_j_sum, f\"Actor log_prob_j_sum {i}\")\n",
        "                actor_loss_i_batch = -q_new + self.alpha * log_prob_i + self.beta * log_prob_j_sum\n",
        "                actor_loss_i = tf.reduce_mean(actor_loss_i_batch)\n",
        "                tf.debugging.check_numerics(actor_loss_i, f\"Actor loss {i}\")\n",
        "                actor_loss_list.append(actor_loss_i)\n",
        "        actor_losses_dict_for_return = {}\n",
        "        for i, agent_id in enumerate(self.agent_ids):\n",
        "            actor_loss_tensor = actor_loss_list[i]\n",
        "            actor_losses_dict_for_return[agent_id] = actor_loss_tensor\n",
        "            actor_vars = self.actors[agent_id].trainable_variables\n",
        "            grads = actor_tape.gradient(actor_loss_tensor, actor_vars)\n",
        "            if grads is not None and all(g is not None for g in grads):\n",
        "                if self.gradient_clip_norm is not None:\n",
        "                    grads, _ = tf.clip_by_global_norm(grads, self.gradient_clip_norm)\n",
        "                self.actor_optimizers[agent_id].apply_gradients(zip(grads, actor_vars))\n",
        "        del actor_tape\n",
        "        return critic_loss, actor_losses_dict_for_return, tf.constant(0.0), tf.constant(0.0)\n",
        "\n",
        "    def update(self):\n",
        "        \"\"\"Samples batch, performs network updates, and soft-updates targets.\"\"\"\n",
        "        if len(self.replay_buffer) < self.batch_size:\n",
        "            return None, None, None, None\n",
        "        sample_result = self.replay_buffer.sample(self.batch_size)\n",
        "        if sample_result is None:\n",
        "            return None, None, None, None\n",
        "        batch_obs, batch_actions, batch_rewards, batch_next_obs, batch_dones = sample_result\n",
        "        try:\n",
        "            update_result = self._update_networks(\n",
        "                batch_obs, batch_actions, batch_rewards, batch_next_obs, batch_dones\n",
        "            )\n",
        "        except tf.errors.InvalidArgumentError as e:\n",
        "            print(f\"\\n!!! Numerical stability error during agent update: {e} !!!\")\n",
        "            return None, None, None, None\n",
        "        if update_result is None:\n",
        "            return None, None, None, None\n",
        "        critic_loss, actor_losses, alpha_loss, beta_loss = update_result\n",
        "        self.update_target_networks()\n",
        "        return critic_loss, actor_losses, alpha_loss, beta_loss\n",
        "\n",
        "    @tf.function(reduce_retracing=True)\n",
        "    def update_target_networks(self):\n",
        "        \"\"\"Performs soft update of target critic networks.\"\"\"\n",
        "        for target_var, source_var in zip(self.target_critic_1.trainable_variables, self.critic_1.trainable_variables):\n",
        "            target_var.assign(self.tau * source_var + (1.0 - self.tau) * target_var)\n",
        "        for target_var, source_var in zip(self.target_critic_2.trainable_variables, self.critic_2.trainable_variables):\n",
        "            target_var.assign(self.tau * source_var + (1.0 - self.tau) * target_var)\n",
        "\n",
        "    def save_weights(self, prefix):\n",
        "        \"\"\"Saves weights for actors and critics.\"\"\"\n",
        "        try:\n",
        "            os.makedirs(prefix, exist_ok=True)\n",
        "            for agent_id, actor in self.actors.items():\n",
        "                actor.save_weights(os.path.join(prefix, f\"actor_{agent_id}.weights.h5\"))\n",
        "            self.critic_1.save_weights(os.path.join(prefix, \"critic1.weights.h5\"))\n",
        "            self.critic_2.save_weights(os.path.join(prefix, \"critic2.weights.h5\"))\n",
        "            print(f\"Weights saved successfully to prefix: {prefix}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving weights with prefix {prefix}: {e}\")\n",
        "\n",
        "    def load_weights(self, prefix):\n",
        "        \"\"\"Loads weights for actors and critics.\"\"\"\n",
        "        try:\n",
        "            for agent_id, actor in self.actors.items():\n",
        "                actor.load_weights(os.path.join(prefix, f\"actor_{agent_id}.weights.h5\"))\n",
        "            self.critic_1.load_weights(os.path.join(prefix, \"critic1.weights.h5\"))\n",
        "            self.critic_2.load_weights(os.path.join(prefix, \"critic2.weights.h5\"))\n",
        "            self.target_critic_1.set_weights(self.critic_1.get_weights())\n",
        "            self.target_critic_2.set_weights(self.critic_2.get_weights())\n",
        "            print(f\"Weights loaded successfully from prefix: {prefix}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading weights from prefix {prefix}: {e}\")\n",
        "\n",
        "# --- Training Function ---\n",
        "def train_masac_v2(env_name=\"simple_spread_v3\", hidden_units=128, actor_lr=1e-4, critic_lr=1e-3,\n",
        "                   tau=0.01, gamma=0.99, buffer_capacity=100000, batch_size=512,\n",
        "                   num_episodes=10000, max_steps_per_episode=100,\n",
        "                   initial_alpha=0.2, initial_beta=0.1, target_entropy_scale=1.0, target_ce_scale=0.1,\n",
        "                   start_steps=5000, update_every=1, log_interval=50,\n",
        "                   gradient_clip_norm=1.0):\n",
        "    \"\"\"Trains the MASACAgentV2 with fixed alpha and beta.\"\"\"\n",
        "    env = make_env(env_name=env_name, continuous_actions=True, max_cycles=max_steps_per_episode)\n",
        "    agent_ids = env.possible_agents\n",
        "    obs_spaces = env.observation_spaces\n",
        "    action_spaces = env.action_spaces\n",
        "    obs_dims = {agent_id: obs_spaces[agent_id].shape[0] for agent_id in agent_ids}\n",
        "    action_dims = {agent_id: action_spaces[agent_id].shape[0] for agent_id in agent_ids}\n",
        "    agent = MASACAgentV2(env, agent_ids, obs_dims, action_dims, action_spaces,\n",
        "                         hidden_units, actor_lr, critic_lr,\n",
        "                         tau, gamma, buffer_capacity, batch_size,\n",
        "                         initial_alpha, initial_beta, target_entropy_scale, target_ce_scale,\n",
        "                         gradient_clip_norm=gradient_clip_norm)\n",
        "    episode_rewards_history = deque(maxlen=100)\n",
        "    total_steps = 0\n",
        "    updates_performed = 0\n",
        "    start_time = time.time()\n",
        "    print(f\"Starting training (V2 Objective) for {num_episodes} episodes...\")\n",
        "    print(f\"Settings: batch_size={batch_size}, start_steps={start_steps}, update_every={update_every}\")\n",
        "    print(f\"LRs: actor={actor_lr}, critic={critic_lr}\")\n",
        "    print(f\"Fixed alpha={initial_alpha}, beta={initial_beta}, tau={tau}, gamma={gamma}, grad_clip={gradient_clip_norm}\")\n",
        "    for episode in range(num_episodes):\n",
        "        try:\n",
        "            observations, infos = env.reset()\n",
        "            episode_reward_sum = 0\n",
        "            terminations = {agent_id: False for agent_id in agent_ids}\n",
        "            truncations = {agent_id: False for agent_id in agent_ids}\n",
        "            steps_in_ep = 0\n",
        "            while not (any(terminations.values()) or any(truncations.values())):\n",
        "                if total_steps < start_steps:\n",
        "                    actions = {agent_id: action_spaces[agent_id].sample() for agent_id in agent.agent_ids}\n",
        "                else:\n",
        "                    valid_obs = {k: v for k, v in observations.items() if k in agent.agent_ids and v is not None}\n",
        "                    if len(valid_obs) < len(agent.agent_ids):\n",
        "                        pass\n",
        "                    actions = agent.get_actions(valid_obs, evaluate=False)\n",
        "                next_observations, rewards, terminations, truncations, infos = env.step(actions)\n",
        "                dones_dict = {agent_id: terminations.get(agent_id, False) or truncations.get(agent_id, False) for agent_id in agent.agent_ids}\n",
        "                reward_dict_ordered = {agent_id: rewards.get(agent_id, 0.0) for agent_id in agent.agent_ids}\n",
        "                action_dict_ordered = {agent_id: actions.get(agent_id, np.zeros(agent.action_dims[agent_id])) for agent_id in agent.agent_ids}\n",
        "                obs_dict_ordered = {agent_id: observations.get(agent_id) for agent_id in agent.agent_ids}\n",
        "                next_obs_dict_ordered = {agent_id: next_observations.get(agent_id) for agent_id in agent.agent_ids}\n",
        "                agent.replay_buffer.push(obs_dict_ordered, action_dict_ordered, reward_dict_ordered, next_obs_dict_ordered, dones_dict)\n",
        "                observations = next_observations\n",
        "                episode_reward_sum += sum(rewards.values())\n",
        "                steps_in_ep += 1\n",
        "                total_steps += 1\n",
        "                if total_steps >= start_steps and total_steps % update_every == 0:\n",
        "                    if len(agent.replay_buffer) >= batch_size:\n",
        "                        num_updates = 1\n",
        "                        for _ in range(num_updates):\n",
        "                            update_result = agent.update()\n",
        "                            if update_result is not None:\n",
        "                                updates_performed += 1\n",
        "                                if updates_performed % log_interval == 0:\n",
        "                                    critic_loss, actor_losses, _, _ = update_result\n",
        "                                    with summary_writer.as_default(step=total_steps):\n",
        "                                        tf.summary.scalar('Loss/Critic_Loss', critic_loss)\n",
        "                                        if isinstance(actor_losses, dict):\n",
        "                                            for ag_id, loss in actor_losses.items():\n",
        "                                                loss_val = loss.numpy() if hasattr(loss, 'numpy') else loss\n",
        "                                                if np.isscalar(loss_val):\n",
        "                                                    tf.summary.scalar(f'Loss/Actor_{ag_id}', loss_val)\n",
        "                                        if episode_rewards_history:\n",
        "                                            avg_rew = np.mean(episode_rewards_history)\n",
        "                                            tf.summary.scalar('Reward/Avg_Episode_Reward_100', avg_rew)\n",
        "                if steps_in_ep >= max_steps_per_episode:\n",
        "                    truncations = {agent_id: True for agent_id in agent.agent_ids}\n",
        "                    break\n",
        "            episode_rewards_history.append(episode_reward_sum)\n",
        "            avg_reward_100 = np.mean(episode_rewards_history) if episode_rewards_history else 0.0\n",
        "            if (episode + 1) % 10 == 0:\n",
        "                elapsed_time = time.time() - start_time\n",
        "                print(f\"Ep {episode+1}: Steps={steps_in_ep}, Avg R (100)={avg_reward_100:.2f}, \"\n",
        "                      f\"Buffer={len(agent.replay_buffer)}, Tot Steps={total_steps}, Time={elapsed_time:.1f}s\")\n",
        "        except Exception as e:\n",
        "            print(f\"\\n!!! Error during episode {episode+1}: {e} !!!\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            continue\n",
        "    total_training_time = time.time() - start_time\n",
        "    print(f\"\\nTraining finished after {num_episodes} episodes in {total_training_time:.2f} seconds.\")\n",
        "    final_avg_reward = np.mean(episode_rewards_history) if episode_rewards_history else 0.0\n",
        "    print(f\"Final Avg Reward (last 100 episodes): {final_avg_reward:.2f}\")\n",
        "    env.close()\n",
        "    return episode_rewards_history, deque(maxlen=100), deque(maxlen=100), total_training_time, agent, log_dir\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(f\"TensorFlow Version: {tf.__version__}\")\n",
        "    # --- Hyperparameters ---\n",
        "    ENV_NAME = \"simple_spread_v3\"\n",
        "    HIDDEN_UNITS = 128\n",
        "    ACTOR_LR = 3e-4\n",
        "    CRITIC_LR = 1e-3\n",
        "    TAU = 0.01\n",
        "    GAMMA = 0.99\n",
        "    BUFFER_CAPACITY = 200000\n",
        "    BATCH_SIZE = 512\n",
        "    NUM_EPISODES = 5000\n",
        "    MAX_STEPS_PER_EPISODE = 100\n",
        "    INITIAL_ALPHA = 0.2\n",
        "    INITIAL_BETA = 0.1\n",
        "    TARGET_ENTROPY_SCALE = 1.0\n",
        "    TARGET_CE_SCALE = 0.1\n",
        "    START_STEPS = 2000\n",
        "    UPDATE_EVERY = 1\n",
        "    LOG_INTERVAL = 100\n",
        "    GRADIENT_CLIP_NORM = 1.0\n",
        "    # --- Start Training ---\n",
        "    tf.get_logger().setLevel('ERROR')\n",
        "    results = None\n",
        "    try:\n",
        "        results = train_masac_v2(\n",
        "            env_name=ENV_NAME, hidden_units=HIDDEN_UNITS, actor_lr=ACTOR_LR, critic_lr=CRITIC_LR,\n",
        "            tau=TAU, gamma=GAMMA, buffer_capacity=BUFFER_CAPACITY, batch_size=BATCH_SIZE,\n",
        "            num_episodes=NUM_EPISODES, max_steps_per_episode=MAX_STEPS_PER_EPISODE,\n",
        "            initial_alpha=INITIAL_ALPHA, initial_beta=INITIAL_BETA, target_entropy_scale=TARGET_ENTROPY_SCALE, target_ce_scale=TARGET_CE_SCALE,\n",
        "            start_steps=START_STEPS, update_every=UPDATE_EVERY, log_interval=LOG_INTERVAL,\n",
        "            gradient_clip_norm=GRADIENT_CLIP_NORM\n",
        "        )\n",
        "    except Exception as main_exception:\n",
        "        print(f\"\\n!!! Critical Error during training: {main_exception} !!!\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "    if results:\n",
        "        episode_rewards_hist, _, _, training_time, trained_agent, save_log_dir = results\n",
        "        print(\"Training run completed.\")\n",
        "        trained_agent.save_weights(os.path.join(save_log_dir, \"final_model\"))\n",
        "    else:\n",
        "        print(\"Training did not complete successfully or was interrupted.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
